# @ashpreetbedi

**founder of [agno](https://github.com/agno-agi/agno) — multi-agent runtime. prev airbnb, facebook.**

17 articles, jan–feb 2026. read chronologically, this is one long pitch disguised as a blog. but inside the pitch are 3–4 ideas that genuinely matter for anyone building agent systems. the trick is separating the signal from the Agno marketing.

---

## the good

**the 3-layer knowledge architecture is the single best idea in this entire corpus — across all five authors.** in [the investment committee piece](why-an-investment-committee.md#three-layer-knowledge-or-stop-putting-everything-in-a-vector-store), he breaks knowledge into three storage strategies: static context always in the prompt (risk limits that must never be missed by search), vector search for large corpora (company profiles where semantic retrieval shines), and file browsing for structured documents read whole (investment memos that lose meaning when chunked). the insight isn't just "use different storage." it's that *where knowledge lives determines how reliably agents use it*. a risk limit in a vector store is a liability. a risk limit in the system prompt is a guarantee. this is the kind of architectural thinking that [@hwchase17](../@hwchase17/README.md) talks around but never crystallizes, and that [@manthanguptaa](../@manthanguptaa/README.md) would reverse-engineer beautifully but hasn't built yet.

**"memory is learning" is a genuine conceptual reframe.** the [founding thesis](memory-is-learning.md#memory-is-learning) — memory is a noun (static database of facts), learning is a verb (evolves, compounds, gets sharper) — changes how you design agent systems. the distinction between extraction (pulling facts from conversation) and integration (teaching the agent to *use* those facts naturally) in [memory-is-learning](memory-is-learning.md#what-it-looks-like-in-action) is the part most memory system builders skip. Claude's memory feels magical not because it extracts well, but because it integrates invisibly. ashpreet names this gap clearly. contrast with [@hwchase17](../@hwchase17/what-is-langsmith-agent-builder.md#how-we-built-our-memory-system)'s filesystem-shaped memory — chase solves the storage shape, ashpreet solves the behavioral loop.

**the investment committee article is a masterclass in multi-agent design.** [five orchestration patterns](why-an-investment-committee.md#the-5-architectures) (route, broadcast, coordinate, task, workflow) applied to the *same 7 agents* with honest trade-offs. the broadcast pattern — independent analysis with no anchoring bias, then chair synthesis — is [the cleanest articulation](why-an-investment-committee.md#broadcast-everyone-votes) of why you'd want agents that can't see each other's work. the workflow pattern — [deterministic pipeline](why-an-investment-committee.md#deterministic-workflow-the-assembly-line) with parallel steps — is what most "multi-agent" tutorials should teach first but don't. if every article were at this level, this would be the best AI engineering blog on the internet.

**the claude code workflow in ["the shift"](the-shift.md#spec-first-development) is immediately actionable.** symlinked specs directory, [layered CLAUDE.md files](the-shift.md#layered-instructions) (repo-level for navigation, feature-level for constraints), [ADR records](the-shift.md#why-this-works) for decision traces, cookbooks-or-it-didn't-happen as the completeness test. the [10-minute PR rule](the-shift.md#the-actual-workflow) enforced in CLAUDE.md is the kind of concrete constraint that actually changes output quality. this is useful regardless of what framework you use. it's also the most honest article — he admits claude writes 100% of his code, that he barely types, and that the specs are also claude-written. the transparency is refreshing.

**the runtime thesis is well-articulated even if self-serving.** [ai-engineering-has-a-runtime-problem](ai-engineering-has-a-runtime-problem.md#why-agents-are-different) names a real gap: agents are stateful, long-running processes forced onto stateless infrastructure. the [stack diagram](ai-engineering-has-a-runtime-problem.md#the-ai-stack) with "runtime" as the missing layer is clean. the argument in [the-architecture-stays-the-same](the-architecture-stays-the-same.md#the-runtime-problem) that the runtime must now *govern behavior* (not just execute code) because agents make decisions is the philosophical dimension most infrastructure pitches miss. the problem is real. the conclusion that only Agno can solve it is the part you should ignore.

## the bad

**the same demo appears in 6 articles and it undermines his credibility.** alice mentions egress costs → bob benefits unprompted → "GPU Poor Learning." it appears in [memory-is-learning](memory-is-learning.md#what-it-looks-like-in-action), [holy-shit-its-working](holy-shit-its-working.md#holy-shit-its-working), [check-this-out](check-this-out.md#check-this-out), [the-problem-with-agents-10](the-problem-with-agents-10.md#the-moment-it-clicked), [what-learning-isnt](what-learning-isnt.md#level-3-knowledge-compounds-across-users), and [first-lets-recap](first-lets-recap.md#namespace-scoping). by article 4, you're not being persuaded — you're being sold to. the learning machine concept deserved one definitive post. [what-learning-isnt](what-learning-isnt.md) comes closest to being that post. the other five are incremental hype for an X audience that probably didn't read the previous ones.

**`learning=True` hides a minefield.** the [demo](what-learning-isnt.md#level-1-the-agent-remembers-you) makes it look like one line of code gives you Claude-quality memory. in practice: background extraction is noisy (he acknowledges this in [holy-shit-its-working](holy-shit-its-working.md#background-vs-agentic) — "potentially noisy"), retrieval hallucinates, and [cross-user knowledge sharing](what-learning-isnt.md#level-3-knowledge-compounds-across-users) is a privacy and security minefield never discussed. what happens when engineer A's proprietary insight about a client surfaces in engineer B's session with a different client? what happens when the vector store fills with contradictory "learnings" from different users? what happens when extraction hallucinates a pattern that doesn't exist and then compounds it across the org? these are the actual hard problems. he criticizes others for the notebook-to-production gap but his own demos have the same gap.

**"GPU Poor Learning" is catchy, misleading, and repeated until it loses meaning.** coined in [check-this-out](check-this-out.md#learning-machines), it's prompt engineering + vector search. comparing it to fine-tuning and RLHF as interchangeable approaches to the same problem misrepresents all three. fine-tuning changes model weights. RLHF changes model preferences. "GPU Poor Learning" changes the context window. these do fundamentally different things. the marketing framing lets him claim "no GPUs harmed!" while glossing over the fact that you're paying for inference on every extraction call, every search, every context injection. it's not GPU-free; it's GPU-shifted.

**every article is a funnel to the same destination.** genuine problem → existing tools fail → Agno solves it. [runtime problem](ai-engineering-has-a-runtime-problem.md#why-this-hasnt-been-solved) → Agno. [eval critique](the-pitch-vs-the-reality.md#what-evals-dont-test) → Agno. [stateless agents](the-problem-with-agents-10.md#the-problem-with-agents-10) → Agno. [own your database](what-we-want-to-build.md#the-case-against-third-party-state) → Agno. [programming language](what-makes-a-programming-language.md#agents-are-the-new-programs) → Agno. the individual arguments often have merit. the destination is always the same. after 17 articles, the pattern is so consistent it functions as a content marketing calendar, not a technical blog.

**"agno is a programming language" is the weakest article and the most revealing.** [the argument](what-makes-a-programming-language.md#what-makes-a-programming-language) — a framework with agents, tools, and workflows constitutes a new language because "every era gets the language it needs." Python didn't call itself a language because it had lists and functions. it earned that label through a grammar, a type system, a memory model, and an execution semantics. Agno is a Python library. calling it a language is the kind of claim that works in a pitch deck but embarrasses in a technical setting. this is where the marketing overcomes the engineering, and it's the article that will age worst.

**the eval critique in [the-pitch-vs-the-reality](the-pitch-vs-the-reality.md) is half-right and half-self-serving.** the argument that "passing evals ≠ working product" is correct. the argument that evals are mostly useless beyond catching regressions is overcorrected. the [sequence he recommends](the-pitch-vs-the-reality.md#the-trap-evals-too-early) (build → ship → learn → eval) is sound but conveniently positions the runtime as the first thing you need, before evals. it's an argument structured to sell infrastructure, not to help engineers make good sequencing decisions. compare [@hwchase17](../@hwchase17/why-you-cant-predict-agent-behavior.md)'s position: continuous production evaluation as a core loop. chase oversells observability the way ashpreet oversells the runtime. the truth is you need both.

## patterns in their thinking

**he's building in public as a go-to-market strategy, and it's working.** the 17 articles are a content engine optimized for X engagement. each introduces a concept (learning machines, runtime, AgentOS), gets engagement, then the next article builds on it. the [investment committee article](why-an-investment-committee.md) at the end is the capstone — it synthesizes everything (learning, orchestration, knowledge layers, governance) into one demo. whether you buy Agno or not, the strategy of "teach the concepts, then show the product" is well-executed.

**he thinks in layers.** [6 layers of context](the-6-layers-of-context.md#the-6-layers-of-context). [3 layers of knowledge](why-an-investment-committee.md#three-layer-knowledge-or-stop-putting-everything-in-a-vector-store). [the AI stack](ai-engineering-has-a-runtime-problem.md#the-ai-stack). [6 types of learning](the-problem-with-agents-10.md#agents-20). his instinct is always to decompose a problem into a stack and then argue one layer is missing. this is a genuinely useful thinking pattern — it forces clarity about what goes where. it's also a marketing pattern — the missing layer is always the one he's selling.

**the "agents are the new programs" thesis is his most ambitious bet.** spanning [the-architecture-stays-the-same](the-architecture-stays-the-same.md#agents-are-the-new-programs), [what-makes-a-programming-language](what-makes-a-programming-language.md#agents-are-the-new-programs), and [why-an-investment-committee](why-an-investment-committee.md#why-five-architectures) — the claim is that software's fundamental unit is shifting from deterministic functions to reasoning agents. if you strip away the Agno pitch, the core observation is correct: when the path between input and output is no longer predetermined, you need new primitives for governance, interaction, and trust. whether Agno is those primitives or whether they emerge at a lower level (in the model itself) is the open question.

**he never discusses cost.** 17 articles about agent infrastructure. not a single mention of token costs, budget constraints, or the economics of running learning machines in production. the [investment committee](why-an-investment-committee.md) runs 7 agents with Opus as the chair. what does that cost per session? per day? per month? [@venturetwins](../@venturetwins/the-great-expansion.md) thinks about monetization constantly. ashpreet writes as if compute is free. this is the biggest gap in his entire body of work.

## worth stealing

1. **[3-layer knowledge](why-an-investment-committee.md#three-layer-knowledge-or-stop-putting-everything-in-a-vector-store)** — critical rules in the prompt (never searched), large corpora in vector search (RAG), structured documents browsed as files (read whole). know which layer each piece of knowledge belongs in. this alone is worth reading all 17 articles for.

2. **[learning modes](the-shift.md#adr-003-learning-modes-always-agentic-propose) as a design decision** — ALWAYS (automatic, invisible), AGENTIC (agent decides via tools), PROPOSE (agent suggests, human approves). not every type of knowledge capture should work the same way. the mode choice changes the product feel entirely.

3. **[spec-first development](the-shift.md#spec-first-development)** — design.md, implementation.md, decisions.md, prompts.md alongside your code. symlink your specs into the repo, gitignore the link. your coding agent reads them. this works regardless of framework.

4. **[broadcast pattern for reducing groupthink](why-an-investment-committee.md#broadcast-everyone-votes)** — send the same question to multiple agents simultaneously, each working independently, then synthesize. no anchoring bias. no self-censoring. the equivalent of a secret ballot before committee deliberation.

5. **[the learning protocol](first-lets-recap.md#the-learning-protocol)** — 5 methods, ~50 lines to extend. the extensibility argument is stronger than the default stores. custom learning stores for your specific domain (legal, medical, sales) are where the real value lives.

6. **[own your database](what-we-want-to-build.md#what-owning-our-database-unlocks)** — the case against third-party state is sound regardless of Agno. zero vendor dependency, full context control, SQL-queryable traces, self-learning loops from your own data.

## what's missing

- **forgetting.** Agno only accumulates. no pruning, decay, archival, contradiction resolution. any system that only remembers and never forgets will drown in noise. [@manthanguptaa](../@manthanguptaa/why-do-current-memory-systems-fail.md#consolidation-and-forgetting) names this problem explicitly — "real intelligence depends on what we choose to forget." ashpreet's learning machine has no forgetting mechanism.

- **self-observation.** his agents learn from user interactions but never examine their own performance. no cost tracking, no efficiency analysis, no behavioral drift detection. the [self-improvement phase](memory-is-learning.md#current-status-and-whats-next) is listed as future work and is the most interesting idea he hasn't built.

- **cost awareness.** what does a 7-agent committee session with Opus as chair actually cost? what's the per-user cost of running learning extraction on every conversation? what's the storage cost of never forgetting? these questions are absent from 17 articles about production infrastructure.

- **honest failure modes.** what happens when [cross-user knowledge transfer](what-learning-isnt.md#level-3-knowledge-compounds-across-users) surfaces something wrong? when the learning store fills with contradictory insights from different users? when extraction hallucinates a "pattern" that doesn't exist and it compounds across sessions? the demos always work. production never does.

- **the elephant in the room: model providers will eat this.** Claude's memory is already magical for consumers. when Anthropic or OpenAI ships a developer-facing learning API with the same quality, what's Agno's moat? the "own your data" argument is real but it's a niche play against the convenience of hosted intelligence. ashpreet knows this — it's why he's racing to build the runtime — but he never discusses it.

---

## articles (chronological)

| date | article | notes |
|------|---------|-------|
| jan 5 | [memory-is-learning](memory-is-learning.md) | the founding thesis. "memory is a noun, learning is a verb." introduces [the learning machine concept](memory-is-learning.md#memory-is-learning) and [7 learning stores](memory-is-learning.md#the-architecture-learning-stores) |
| jan 7 | [holy-shit-its-working](holy-shit-its-working.md) | the egress cost demo (appearance 1 of 6). [background vs agentic mode](holy-shit-its-working.md#background-vs-agentic) is the most honest section |
| jan 8 | [first-lets-recap](first-lets-recap.md) | technical deep dive on [the learning protocol](first-lets-recap.md#the-learning-protocol). strongest technical writing. [5 functions, ~50 lines](first-lets-recap.md#build-your-own-learning-store) |
| jan 10 | [the-pitch-vs-the-reality](the-pitch-vs-the-reality.md) | anti-eval take. [what evals don't test](the-pitch-vs-the-reality.md#what-evals-dont-test) has merit; conclusion is self-serving |
| jan 12 | [check-this-out](check-this-out.md) | "GPU Poor Learning" coined. [extensible protocol](check-this-out.md#the-protocol) is the best argument |
| jan 13 | [the-shift](the-shift.md) | claude code workflow. [spec-first development](the-shift.md#spec-first-development) and [layered CLAUDE.md](the-shift.md#layered-instructions) are genuinely useful |
| jan 15 | [ai-engineering-has-a-runtime-problem](ai-engineering-has-a-runtime-problem.md) | the [runtime thesis](ai-engineering-has-a-runtime-problem.md#why-agents-are-different). real problem, obvious product placement |
| jan 21 | [the-problem-with-agents-10](the-problem-with-agents-10.md) | [agents 1.0 vs 2.0](the-problem-with-agents-10.md#agents-20). egress demo appearance 3 |
| jan 21 | [the-path-to-production](the-path-to-production.md) | [AgentOS launch](the-path-to-production.md#introducing-agentos). 50+ API endpoints, JWT RBAC, control plane |
| jan 26 | [what-we-want-to-build](what-we-want-to-build.md) | ["own your database"](what-we-want-to-build.md#the-case-against-third-party-state) — strongest argument for self-hosted state |
| jan 28 | [what-learning-isnt](what-learning-isnt.md) | most complete learning machine article. [3 levels](what-learning-isnt.md#level-1-the-agent-remembers-you) of progressive complexity |
| jan 29 | [what-it-does](what-it-does.md) | Pal personal agent. [creates tables on the fly](what-it-does.md#what-makes-it-different) — ambitious, fragile |
| jan 29 | [the-agents](the-agents.md) | AgentOS template walkthrough. [run locally in 5 min](the-agents.md#run-locally-5-minutes) |
| feb 1 | [the-6-layers-of-context](the-6-layers-of-context.md) | Dash text-to-SQL agent. [self-learning loop](the-6-layers-of-context.md#the-self-learning-loop) applied to data agents |
| feb 12 | [the-architecture-stays-the-same](the-architecture-stays-the-same.md) | ["agents are the new programs"](the-architecture-stays-the-same.md#agents-are-the-new-programs). when software [makes decisions](the-architecture-stays-the-same.md#when-software-makes-decisions), governance enters the stack |
| feb 17 | [what-makes-a-programming-language](what-makes-a-programming-language.md) | "agno is a language." [the argument](what-makes-a-programming-language.md#what-makes-a-programming-language) doesn't hold. peak overreach |
| feb 19 | [why-an-investment-committee](why-an-investment-committee.md) | best article. [5 orchestration patterns](why-an-investment-committee.md#the-5-architectures), [3-layer knowledge](why-an-investment-committee.md#three-layer-knowledge-or-stop-putting-everything-in-a-vector-store), [institutional learning](why-an-investment-committee.md#institutional-learning). capstone piece |
