---
title: "There is no God Tier video model: But there is something better"
author: Justine Moore
date: 2025-10-28
url: https://a16z.com/there-is-no-god-tier-video-model-but-there-is-something-better/
source: a16z
---

# There is no God Tier video model: But there is something better

Last year, it seemed like every week a new frontier video model would one-up its peers and push our expectations. We learned to simply expect relentless improvement on measurable benchmarks: longer video output, sustained stories, better physics, character consistency, movement cohesion, and more.

But this year, if you care about the benchmarks a lot, you might be getting a little restless. We pretty much expect all video models to generate 10-15 seconds with synced audio and speech, and an all-around sense of realism — which is a great achievement, but not the runaway model progress to which we'd been conditioned.

**Instead, we're entering a more exciting time: the product era of video models, where "abundance" isn't about benchmark performance — it's about variety.**

Key observations:
- We're beginning to see models specialize across specific dimensions: there is no "God Model" that's great at everything
- Startups are finding new opportunities across two main dimensions: models that excel at one key thing, and products that abstract away arduous workflows
- **Hot take: even if video model progress stopped entirely, founders would be playing years of catch-up building products around current model capabilities**
- This is great for startups: it creates space for verticals and wedges that can become their own massive companies

## A Brief History of Models

For the past few years, frontier diffusion model labs consistently released models that outperform prior generations on major benchmarks. This progress inculcated an assumption: a "god-tier" model that is great at everything would eventually emerge.

But this year, the assumption was challenged: Sora 2 was released and appeared below Veo 3 [on benchmarks like LMarena](https://lmarena.ai/leaderboard/text-to-video/overall). Progress may be slowing on the diffusion model level, and the concept of "SOTA" for video models may not actually exist.

This is not unique to diffusion models. Frontier LLMs also followed a pattern of step-function improvements from 2023-2025, then saw performance begin to level off. When this happened with LLMs, we saw foundation labs begin to specialize and the AI product layer take off.

A good analogy: still-life painting in the 17th and 18th century. At a certain point, the old masters got so good at painting realistic portraits that preferring one artist over another came down to aesthetic taste, rather than any objective measure of what looked more "real."

So what happens when realism stops being the differentiator? We get more variety and specialization. **Welcome to the abundance era.**

## The Models are Specializing

- **[Veo 3](https://deepmind.google/models/veo/):** strongest handle on physics, complex movement, and audio & speech synced with video
- **[Sora 2](https://sora.chatgpt.com/explore):** will "direct" for you from a short prompt, writing the script (often funny!) and creating multiple shots
- **[Wan](https://wan.video/):** solid open source player with an ecosystem of LoRAs for specific styles, motions, or effects
- **[Grok](https://grok.com/imagine):** fast, inexpensive, particularly great at anime and animation
- **[Seedance Pro](https://higgsfield.ai/seedance):** can produce multi-shot scenes in a single generation
- **[Hedra](https://www.hedra.com/):** great at long-form clips of talking characters

Comparing outputs from Sora 2 and Veo 3 is a great illustration. Both are exceptional video models, but good at very different things. **Sora excels at inserting yourself into a scene and crafting a story from a short prompt** — great for consumers and meme makers. **Veo 3 is a more powerful and controllable model for prosumer and professional creatives** — less of a sense of humor but better at physics and complex motion.

This trend of model specialization is also positive for players up and down the stack. AI video cloud providers like [Fal](https://fal.ai/) and [Replicate](https://replicate.com/) now host dozens of models for users seeking to access various vertical use-cases. Editing suites like [Krea](https://www.krea.ai/) give users a central hub to interact with multiple models and build entire workflows.

## Products for the Masses

Creators are hacking together complex workflows across multiple products to do things that could feasibly be done by the models — getting [consistent characters](https://x.com/PJaccetturo/status/1974576539225485751) across generations, [extending a scene](https://x.com/venturetwins/status/1978294621416440076) by taking the end frame of the last clip, [controlling camera movement](https://x.com/venturetwins/status/1964721544791466367) via start and end frames using an image edit model, or [collaging storyboards](https://x.com/0xFramer/status/1975945198992802022).

The good news is some labs are starting to address this product gap:
- Runway released [a suite of apps](https://runwayml.com/research/introducing-runway-aleph) that enable creators to edit camera angles, do next-shot video generation, perform style transfers, and add or remove items from clips
- [Sora Storyboard](https://www.youtube.com/watch?v=6PXWAvUG8Sg) enables users to specify precise moment-to-moment action in a video sequence
- Veo 3.1 consisted almost entirely of product updates around audio and visual control, rather than model-level improvements

Some people say if LLM progress stalled tomorrow, entrepreneurs would still have years of catchup building useful products. **The same is true of video models** — we're only just beginning to see end-to-end products being built around these models.
